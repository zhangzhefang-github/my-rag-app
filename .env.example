# .env.example - RAG Demo Configuration Template
# Copy this file to .env and fill in your specific values.

# --- LLM Provider Selection ---
# Select the backend LLM service. Options: 'ollama', 'custom_api', 'openai'
LLM_PROVIDER=ollama

# --- Ollama Configuration (if LLM_PROVIDER=ollama) ---
OLLAMA_HOST=http://localhost:11434       # Your Ollama service URL
OLLAMA_MODEL=qwen3:0.6b                 # Model name available in Ollama (e.g., run 'ollama list')
# OLLAMA_TEMPERATURE=0.7                 # (Optional) Sampling temperature
# OLLAMA_NUM_PREDICT=256                # (Optional) Max tokens for Ollama

# --- Custom API Configuration (if LLM_PROVIDER=custom_api) ---
# CUSTOM_API_KEY="sk-your_custom_key"       # API Key for your custom endpoint
# CUSTOM_API_BASE="https://api.fe8.cn/v1"   # Base URL for your custom endpoint
# CUSTOM_API_MODEL="gpt-4o-mini"            # Model name available at the custom endpoint
# CUSTOM_API_TEMPERATURE=0.7             # (Optional)
# CUSTOM_API_MAX_TOKENS=1024              # (Optional)

# --- OpenAI Configuration (if LLM_PROVIDER=openai) ---
# Uses the official OpenAI API via LangChain.
# Requires OPENAI_API_KEY to be set. Base URL is optional.
# OPENAI_API_KEY="sk-your_openai_key"       # Your official OpenAI API Key
# OPENAI_API_BASE="https://api.openai.com/v1" # (Optional) Use if you have a proxy
# OPENAI_MODEL="gpt-4o-mini"                # Official OpenAI model name
# OPENAI_TEMPERATURE=0.7                 # (Optional)
# OPENAI_MAX_TOKENS=1024                  # (Optional)

# --- Retrieval & Embedding Settings ---\n# EMBEDDING_PROVIDER=\"huggingface\"  # (Optional) Choose embedding provider: huggingface, openai, ollama. Defaults to huggingface if not set or based on model name format.
RETRIEVER_MODEL="moka-ai/m3e-base"  # Embedding model path (HuggingFace) or name (OpenAI/Ollama)
# OPENAI_API_KEY=\"sk-your_openai_key\" # Required if EMBEDDING_PROVIDER=openai or LLM_PROVIDER=openai (reuse key)
# OLLAMA_HOST=\"http://localhost:11434\" # Required if EMBEDDING_PROVIDER=ollama (can reuse OLLAMA_HOST from LLM section)
# OLLAMA_EMBED_MODEL=\"nomic-embed-text\" # Required if EMBEDDING_PROVIDER=ollama, specific model for embedding

# --- Settings specific to HuggingFace Embedder ---
LOCAL_MODEL_DIR="models"              # Directory to cache embedding models (if using HuggingFace)
RETRIEVER_USE_GPU=true                # (Optional) Attempt to use GPU for HuggingFace embeddings (default: true)

# --- General Retriever Settings ---
TOP_K=3                               # Number of document chunks to retrieve per query
INDEX_DIR="data/indexes"              # Directory to store Faiss index and doc mappings
DOCS_DIR="data/documents"             # Directory containing source documents

# --- System Settings ---
LOG_LEVEL="INFO"                      # Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
APP_PORT=8000                         # Port for the FastAPI API server